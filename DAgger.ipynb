{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReachTarget' object has no attribute '_task'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ddec671395e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0ddec671395e>\u001b[0m in \u001b[0;36mcompute_action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_servo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_joint_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0ddec671395e>\u001b[0m in \u001b[0;36mp_servo\u001b[0;34m(self, gain)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mwTe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfkine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mwTt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_pose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Pose difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0ddec671395e>\u001b[0m in \u001b[0;36mtarget_pose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtarget_pose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Target pose in world coordinate frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mwTt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSE3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSE3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_orientation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwTt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReachTarget' object has no attribute '_task'"
     ]
    }
   ],
   "source": [
    "\n",
    "# DAgger Implementation for RLBench\n",
    "# Paper: https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf\n",
    "# Author: Krishan Rana\n",
    "\n",
    "from rlbench.environment import Environment\n",
    "from rlbench.action_modes import ArmActionMode, ActionMode\n",
    "from rlbench.observation_config import ObservationConfig\n",
    "from rlbench.tasks import ReachTarget, PushButtons\n",
    "import pdb\n",
    "import numpy as np\n",
    "from spatialmath import SE3, SO3\n",
    "import pdb\n",
    "import roboticstoolbox as rb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class RRMC():\n",
    "    def __init__(self):\n",
    "        self.panda = rb.models.DH.Panda()\n",
    "\n",
    "    def fkine(self):\n",
    "        # Tip pose in world coordinate frame\n",
    "        wTe = SE3(env._task.robot.arm.get_tip().get_position())*SE3.Eul(env._task.robot.arm.get_tip().get_orientation())\n",
    "        return wTe\n",
    "    \n",
    "    def target_pose(self):\n",
    "        # Target pose in world coordinate frame\n",
    "        wTt = SE3(env._task.target.get_position())*SE3.Eul(env._task.target.get_orientation())\n",
    "        return wTt\n",
    "    \n",
    "    def p_servo(self, gain=1):\n",
    "        \n",
    "        wTe = self.fkine();\n",
    "        wTt = self.target_pose();\n",
    "    \n",
    "        # Pose difference\n",
    "        eTt = wTe.inv() * wTt\n",
    "        # Translational velocity error\n",
    "        ev = eTt.t\n",
    "        # Angular velocity error\n",
    "        ew = eTt.rpy() * np.pi/180\n",
    "\n",
    "        # Form error vector\n",
    "        e = np.r_[ev, ew]\n",
    "        v = gain * e\n",
    "        return v\n",
    "    \n",
    "    def compute_action(self):\n",
    "        \n",
    "        try:\n",
    "            v = self.p_servo(gain=0.3)\n",
    "            v[3:] *= 10\n",
    "            q = env._task.robot.arm.get_joint_positions()\n",
    "            action = np.linalg.pinv(self.panda.jacobe(q)) @ v\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            action = np.zeros(env_task.action_size)\n",
    "            print('Fail')\n",
    "\n",
    "        return action\n",
    "        \n",
    "        \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, act_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(2704, 520)\n",
    "        self.fc2 = nn.Linear(520,108)\n",
    "        self.fc3 = nn.Linear(108,54)\n",
    "        self.fc4 = nn.Linear(54, act_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pi = Model(env_task.action_size).to(device)\n",
    "        \n",
    "    def train(self):\n",
    "        print(\"Training...\")\n",
    "        batch = random.sample(experience_dataset, 32)\n",
    "        batch_obs = torch.as_tensor([demo[1] for demo in batch], dtype=torch.float32).to(device).permute(0,3,1,2)\n",
    "        predicted_actions = self.pi(batch_obs)\n",
    "        ground_truth_actions = torch.as_tensor([demo[0] for demo in batch], dtype=torch.float32).to(device).detach()\n",
    "        loss = criterion(predicted_actions, ground_truth_actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs = torch.as_tensor(obs, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "            obs = obs.permute(0,3,1,2)\n",
    "            act = self.pi(obs).cpu().numpy()[0]\n",
    "            return act\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "obs_config = ObservationConfig()\n",
    "obs_config.set_all(False)\n",
    "obs_config.wrist_camera.rgb = True\n",
    "obs_config.joint_positions  = True\n",
    "\n",
    "action_mode = ActionMode(ArmActionMode.ABS_JOINT_VELOCITY)\n",
    "env_task = Environment(action_mode, '', obs_config, False)\n",
    "env_task.launch()\n",
    "\n",
    "env = env_task.get_task(ReachTarget)\n",
    "agent = Agent()\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "control_prior = RRMC()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(agent.pi.parameters(), lr=0.01, momentum=0.9)\n",
    "total_steps = 1000\n",
    "episode_length = 40\n",
    "experience_dataset = []\n",
    "\n",
    "descriptions, state = env.reset()\n",
    "obs = state.wrist_rgb\n",
    "\n",
    "for i in range(total_steps):\n",
    "    if i%episode_length == 0 and i > 100:\n",
    "        descriptions, state = env.reset()\n",
    "        agent.train()\n",
    "    action = agent.get_action(obs)\n",
    "    action = control_prior.compute_action()\n",
    "    pdb.set_trace()\n",
    "    next_state, reward, done = env.step(action)\n",
    "    nobs = state.wrist_rgb\n",
    "    #experience_dataset.append([control_prior.act(obs), obs])\n",
    "    experience_dataset.append([action, obs])\n",
    "    obs = nobs\n",
    "    \n",
    "print('Done')\n",
    "env.shutdown()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
