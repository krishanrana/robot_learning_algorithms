{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rlbench'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41fb54e3adc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Author: Krishan Rana\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrlbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrlbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_modes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArmActionMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActionMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrlbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mObservationConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rlbench'"
     ]
    }
   ],
   "source": [
    "\n",
    "# DAgger Implementation for RLBench\n",
    "# Paper: https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf\n",
    "# Author: Krishan Rana\n",
    "\n",
    "from rlbench.environment import Environment\n",
    "from rlbench.action_modes import ArmActionMode, ActionMode\n",
    "from rlbench.observation_config import ObservationConfig\n",
    "from rlbench.tasks import ReachTarget, PushButtons\n",
    "import numpy as np\n",
    "import pdb\n",
    "import numpy as np\n",
    "from spatialmath import SE3, SO3\n",
    "import pdb\n",
    "import roboticstoolbox as rb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class RRMC():\n",
    "    def __init__(self):\n",
    "        self.panda = rb.models.DH.Panda()\n",
    "\n",
    "    def fkine(self):\n",
    "        # Tip pose in world coordinate frame\n",
    "        wTe = SE3(env._task.robot.arm.get_tip().get_position())*SE3.Eul(env._task.robot.arm.get_tip().get_orientation())\n",
    "        return wTe\n",
    "    \n",
    "    def target_pose(self):\n",
    "        # Target pose in world coordinate frame\n",
    "        wTt = SE3(env._task.target.get_position())*SE3.Eul(env._task.target.get_orientation())\n",
    "        return wTt\n",
    "    \n",
    "    def p_servo(self, gain=1):\n",
    "        \n",
    "        wTe = fkine();\n",
    "        wTt = target_pose();\n",
    "    \n",
    "        # Pose difference\n",
    "        eTt = wTe.inv() * wTt\n",
    "        # Translational velocity error\n",
    "        ev = eTt.t\n",
    "        # Angular velocity error\n",
    "        ew = eTt.rpy() * np.pi/180\n",
    "\n",
    "        # Form error vector\n",
    "        e = np.r_[ev, ew]\n",
    "        v = gain * e\n",
    "        return v\n",
    "    \n",
    "    def compute_action(self):\n",
    "        \n",
    "        try:\n",
    "            v = self.p_servo(gain=0.3)\n",
    "            v[3:] *= 10\n",
    "            q = env._target.robot.arm.get_joint_positions()\n",
    "            action = np.linalg.pinv(self.panda.jacobe(q)) @ v\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            action = np.zeros(env_task.action_size)\n",
    "            print('Fail')\n",
    "\n",
    "        return action\n",
    "        \n",
    "        \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, act_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(2704, 520)\n",
    "        self.fc2 = nn.Linear(520,108)\n",
    "        self.fc3 = nn.Linear(108,54)\n",
    "        self.fc4 = nn.Linear(54, act_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pi = Model(env_task.action_size).to(device)\n",
    "        \n",
    "    def train(self):\n",
    "        print(\"Training...\")\n",
    "        batch = random.sample(experience_dataset, 32)\n",
    "        batch_obs = torch.as_tensor([demo[1] for demo in batch], dtype=torch.float32).to(device).permute(0,3,1,2)\n",
    "        predicted_actions = self.pi(batch_obs)\n",
    "        ground_truth_actions = torch.as_tensor([demo[0] for demo in batch], dtype=torch.float32).to(device).detach()\n",
    "        loss = criterion(predicted_actions, ground_truth_actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs = torch.as_tensor(obs, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "            obs = obs.permute(0,3,1,2)\n",
    "            act = self.pi(obs).cpu().numpy()[0]\n",
    "            return act\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "obs_config = ObservationConfig()\n",
    "obs_config.set_all(False)\n",
    "obs_config.wrist_camera.rgb = True\n",
    "obs_config.joint_positions  = True\n",
    "\n",
    "action_mode = ActionMode(ArmActionMode.ABS_JOINT_VELOCITY)\n",
    "env_task = Environment(action_mode, '', obs_config, False)\n",
    "env_task.launch()\n",
    "\n",
    "env = env_task.get_task(ReachTarget)\n",
    "agent = Agent()\n",
    "\n",
    "pdb.set_trace()\n",
    "\n",
    "obs = env.reset()\n",
    "control_prior = RRMC()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(agent.pi.parameters(), lr=0.01, momentum=0.9)\n",
    "total_steps = 1000\n",
    "episode_length = 40\n",
    "experience_dataset = []\n",
    "\n",
    "descriptions, state = env.reset()\n",
    "obs = state.wrist_rgb\n",
    "\n",
    "for i in range(total_steps):\n",
    "    if i%episode_length == 0 and i > 100:\n",
    "        descriptions, state = env.reset()\n",
    "        agent.train()\n",
    "    action = agent.get_action(obs)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    nobs = state.wrist_rgb\n",
    "    demos.append([control_prior.act(obs), obs])\n",
    "    experience_dataset.append([action, obs])\n",
    "    obs = nobs\n",
    "    \n",
    "print('Done')\n",
    "env.shutdown()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
